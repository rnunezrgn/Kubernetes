---
- name: Kubernetes Cluster Setup (All Nodes)
  hosts: rhel9k8s
  become: true
  vars:
    pod_network_cidr: "10.216.0.0/16"

  tasks:
    # RHEL Registration & Repos
    - name: Register RHEL subscription
      community.general.redhat_subscription:
        username: "{{ myusername }}"
        password: "{{ mypassword }}"
        auto_attach: true
        state: present

    - name: Enable required RHEL repos
      ansible.builtin.shell: |
        subscription-manager repos --enable=rhel-9-for-x86_64-baseos-rpms \
                                   --enable=rhel-9-for-x86_64-appstream-rpms \
                                   --enable codeready-builder-for-rhel-9-x86_64-rpms

    - name: Ensure redhat.repo file is present
      ansible.builtin.stat:
        path: /etc/yum.repos.d/redhat-rhui.repo
      register: redhat_rhui_repo

    - name: Fail if redhat.repo does not exist
      ansible.builtin.fail:
        msg: "redhat.repo file is missing. Repositories may not be enabled correctly."
      when: not redhat_rhui_repo.stat.exists

    # System Configuration
    - name: Disable SELinux
      ansible.posix.selinux:
        state: disabled

    - name: Disable swap
      ansible.builtin.shell: |
        swapoff -a
        sed -i.bak '/ swap /s/^/#/' /etc/fstab

    # Load Kernel modules 
    - name: Load br_netfilter, ip_vs, ip_vs_rr, ip_vs_wrr, ip_vs_sh and overlay kernel modules
      community.general.modprobe:
        name: "{{ item }}"
        state: present
      loop:
        - br_netfilter
        - ip_vs
        - ip_vs_rr
        - ip_vs_wrr
        - ip_vs_sh
        - overlay

    - name: Ensure br_netfilter, ip_vs, ip_vs_rr, ip_vs_wrr, ip_vs_sh and overlay loads on boot
      ansible.builtin.copy:
      #  dest: /etc/modules-load.d/br_netfilter.conf
        dest: /etc/modules-load.d/kubernetes.conf
        content: |
          br_netfilter
          ip_vs
          ip_vs_rr
          ip_vs_wrr
          ip_vs_sh
          overlay
        mode: '0644'

    - name: Configure sysctl to enable IP forwarding and configure Kubernetes networking
      ansible.builtin.copy:
        dest: /etc/sysctl.d/kubernetes.conf
        content: |
          net.ipv4.ip_forward = 1
          net.bridge.bridge-nf-call-ip6tables = 1
          net.bridge.bridge-nf-call-iptables = 1
        mode: '0644'

    - name: Apply sysctl settings
      command: sysctl --system

    # Install Required Packages
    - name: Import EPEL GPG key
      ansible.builtin.rpm_key:
        state: present
        key: https://dl.fedoraproject.org/pub/epel/RPM-GPG-KEY-EPEL-9

    - name: Install EPEL release
      ansible.builtin.dnf:
        name: "https://dl.fedoraproject.org/pub/epel/epel-release-latest-9.noarch.rpm"
        state: present
        disablerepo:
        - kubernetes
        - crio-o

    - name: Add Kubernetes repo
      ansible.builtin.copy:
        dest: /etc/yum.repos.d/kubernetes.repo
        content: |
          [kubernetes]
          name=Kubernetes
          baseurl=https://pkgs.k8s.io/core:/stable:/v1.29/rpm/
          enabled=1
          gpgcheck=1
          repo_gpgcheck=1
          gpgkey=https://pkgs.k8s.io/core:/stable:/v1.29/rpm/repodata/repomd.xml.key

#    - name: Clean yum cache
#      ansible.builtin.command: dnf clean all

    - name: Install Kubernetes tools
      ansible.builtin.dnf:
        name:
          - kernel-devel-5.14.0-570.32.1.el9_6.x86_64
          - lsof 
          - kubelet
          - kubeadm
          - kubectl
          - podman
          - buildah
          - skopeo
          - epel-release
        state: present

    - name: Enable kubelet
      ansible.builtin.shell: systemctl enable --now kubelet

    - name: Ensure docker-ce-repo file is present
      ansible.builtin.stat:
        path: /etc/yum.repos.d/docker-ce.repo
      register: my_docker_ce_repo

    - name: Add Docker CE repository (provides containerd.io) if /etc/yum.repos.d/docker-ce.repo file does not exist
#      ansible.builtin.shell: dnf config-manager --add-repo=https://download.docker.com/linux/rhel/docker-ce.repo
      ansible.builtin.shell: dnf config-manager --add-repo=https://download.docker.com/linux/rhel/9/x86_64/stable
      when: not my_docker_ce_repo.stat.exists

    - name: Import Docker GPG key (still from 'centos' path)
      ansible.builtin.rpm_key:
        key: https://download.docker.com/linux/centos/gpg
        state: present

    - name: Install containerd
      ansible.builtin.dnf:
        name: containerd.io
        state: present
          
    - name: Configure containerd
      ansible.builtin.shell: |
        mkdir -p /etc/containerd
        containerd config default > /etc/containerd/config.toml
        sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml
        systemctl daemon-reload
        systemctl restart containerd
        systemctl enable --now containerd.service
        
    - name: Unmask and enable kubelet
      ansible.builtin.systemd:
        name: kubelet
        masked: no
        enabled: yes
        state: started
      register: my_kubelet_status

    - name: Debuging my_kubelet_status
      ansible.builtin.debug:
        msg: "This is the Kubelet status: {{ my_kubelet_status }}" 

    - name: Check containerd statis
      ansible.builtin.shell: systemctl status containerd.service
      register: my_containerd_status

    - name: Debuging my_containerd_status
      ansible.builtin.debug:
        msg: "This is the Containerd status: {{ my_containerd_status }}" 

# --- Control Plane Initialization ---
- name: Initialize Kubernetes Control Plane
  hosts: k8smasters
  become: true
  vars:
    pod_network_cidr: "10.216.0.0/16"

  tasks:
    - name: Check config image pull on K8MASTERS
      ansible.builtin.shell: kubeadm config images pull
      register: my_images

    - name: Debuging my_images
      ansible.builtin.debug:
        msg: "This is the Kube Images status: {{ my_images }}" 

    - name: Check if cluster already initialized
      ansible.builtin.stat:
        path: /etc/kubernetes/admin.conf
      register: k8s_initialized

    - name: Initialize Kubernetes cluster
      ansible.builtin.command: kubeadm init --pod-network-cidr={{ pod_network_cidr }}
      when: not k8s_initialized.stat.exists
      register: kubeadm_output

    - name: Setup kubeconfig for root
      ansible.builtin.shell: |
        mkdir -p /root/.kube
        cp -i /etc/kubernetes/admin.conf /root/.kube/config
        chown root:root /root/.kube/config
      when: not k8s_initialized.stat.exists

    - name: Setup kubeconfig for ec2-user
      become_user: ec2-user
      ansible.builtin.shell: |
        mkdir -p /home/ec2-user/.kube
        cp /etc/kubernetes/admin.conf /home/ec2-user/.kube/config
        chown ec2-user:ec2-user /home/ec2-user/.kube/config
      when: not k8s_initialized.stat.exists

    # --- Update Flannel CNI Configuration ---
#    - name: Update Flannel CNI configuration with pod_network_cidr
#      ansible.builtin.copy:
#        dest: /etc/cni/net.d/10-flannel.conflist
#        content: |
#          {
#            "name": "cbr0",
#            "cniVersion": "0.3.1",
#            "plugins": [
#              {
#                "type": "flannel",
#                "delegate": {
#                  "hairpinMode": true,
#                  "isDefaultGateway": true,
#                  "ipMasq": true,
#                  "ipam": {
#                    "type": "host-local",
#                    "subnet": "{{ pod_network_cidr }}",  # Use pod_network_cidr from vars
#                    "rangeStart": "10.216.0.10",         # Starting IP for pods
#                    "rangeEnd": "10.216.255.254",       # Ending IP for pods
#                    "gateway": "10.216.0.1"             # Gateway for the network
#                  }
#                }
#              },
#              {
#                "type": "portmap",
#                "capabilities": {
#                  "portMappings": true
#                }
#              }
#            ]
#          }
#        mode: '0644'

    - name: Deploy Flannel CNI
      ansible.builtin.shell: kubectl apply -f https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml
   #   kubectl create -f https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml
   #   when: not k8s_initialized.stat.exists

    - name: Wait for Flannel pods to be running on all nodes
      ansible.builtin.shell: kubectl get pods -n kube-system -l app=flannel --no-headers | grep -v Running && exit 1 || exit 0
      register: flannel_ready
      retries: 10
      delay: 10
      until: flannel_ready.rc == 0
      when: not k8s_initialized.stat.exists

    - name: Wait for Kubernetes API to become available
      ansible.builtin.uri:
        url: https://localhost:6443/healthz
        method: GET
        status_code: 200
        validate_certs: false
      register: result
      until: result.status == 200
      retries: 10
      delay: 10

    - name: Debuging API result variable
      ansible.builtin.debug:
        msg: "This is the Kubernetes API status: {{ result }}" 

    - name: Upload certificate key (for control-plane join if needed)
      ansible.builtin.shell: kubeadm init phase upload-certs --upload-certs
      register: cert_key_output
      when: not k8s_initialized.stat.exists
      changed_when: false

    - name: Generate kubeadm join command with cert key
      ansible.builtin.shell: |
        kubeadm token create --print-join-command --certificate-key {{ cert_key_output.stdout_lines[-1] }}
      register: join_cmd
      when: not k8s_initialized.stat.exists

    - name: Get join command from control plane
      ansible.builtin.command: kubeadm token create --print-join-command
      register: join_cmd
      changed_when: false

    - name: Debuging join_cmd
      ansible.builtin.debug:
        msg: "This is the JOIN_CMD result: {{ join_cmd }}" 
    
    - name: Set join command fact
      ansible.builtin.set_fact:
        join_command: "{{ join_cmd.stdout }}"

# --- Worker Node Join ---
- name: Join Worker Nodes to the Cluster
  hosts: k8sworkers
  become: true
  vars:
    join_command: "{{ hostvars[groups['k8smasters'][0]].join_command }}"

  tasks:
    - name: Load br_netfilter module immediately
      ansible.builtin.modprobe:
        name: br_netfilter
        state: present

    - name: Ensure br_netfilter loads on boot
      ansible.builtin.copy:
        dest: /etc/modules-load.d/br_netfilter.conf
        content: "br_netfilter\n"
        mode: '0644'

    - name: Configure sysctl for Kubernetes networking
      ansible.builtin.copy:
        dest: /etc/sysctl.d/k8s.conf
        content: |
          net.bridge.bridge-nf-call-ip6tables = 1
          net.bridge.bridge-nf-call-iptables = 1
          net.ipv4.ip_forward = 1
        mode: '0644'

    - name: Apply sysctl settings
      ansible.builtin.command: sysctl --system

#    - name: Ensure kubelet is enabled and started
#      ansible.builtin.systemd:
#        name: kubelet
#        enabled: true
#        state: started
#        masked: no

    - name: Stop and mask kubelet to free port 10250
      ansible.builtin.systemd:
        name: kubelet
        state: stopped
        enabled: no
        masked: yes
        
    - name: Reset previous kubeadm state
      ansible.builtin.shell: |
        kubeadm reset -f
        rm -rf /etc/kubernetes/
        rm -rf /var/lib/kubelet/*
        rm -rf /var/lib/cni/
        rm -rf /etc/cni/
        systemctl daemon-reexec
        systemctl restart containerd
      ignore_errors: yes
    
    - name: Join node to cluster
      ansible.builtin.shell: "{{ join_command }} --ignore-preflight-errors=Hostname"
      args:
        executable: /bin/bash
      register: join_result
      failed_when: "'[ERROR' in join_result.stderr"

    - name: Debuging join_result variable
      ansible.builtin.debug:
        msg: "This is the JOIN_RESULT variable: {{ join_result }}" 

    - name: Run Kubectl get nodes before unmarks and enable kubelet
      ansible.builtin.shell: kubectl get nodes
      register: my_get_nodes

    - name: Debuging my_get_nodes variable
      ansible.builtin.debug:
        msg: "This is the MY_GET_NODES variable: {{ my_get_nodes }}" 
      
    - name: Unmask and enable kubelet after join
      ansible.builtin.systemd:
        name: kubelet
        state: restarted
        enabled: yes
        masked: no

    # --- Update Flannel CNI Configuration ---
#    - name: Update Flannel CNI configuration with pod_network_cidr
#      ansible.builtin.copy:
#        dest: /etc/cni/net.d/10-flannel.conflist
#        content: |
#          {
#            "name": "cbr0",
#            "cniVersion": "0.3.1",
#            "plugins": [
#              {
#                "type": "flannel",
#                "delegate": {
#                  "hairpinMode": true,
#                  "isDefaultGateway": true,
#                  "ipMasq": true,
#                  "ipam": {
#                    "type": "host-local",
#                    "subnet": "{{ pod_network_cidr }}",  # Use pod_network_cidr from vars
#                    "rangeStart": "10.216.0.10",         # Starting IP for pods
#                    "rangeEnd": "10.216.255.254",       # Ending IP for pods
#                    "gateway": "10.216.0.1"             # Gateway for the network
#                  }
#                }
#              },
#              {
#                "type": "portmap",
#                "capabilities": {
#                  "portMappings": true
#                }
#              }
#            ]
#          }
#        mode: '0644'

    # --- Redeploy Kubernetes Pods ---
    - name: Wait for kube-apiserver to be available
      ansible.builtin.uri:
        url: https://localhost:6443/healthz
        method: GET
        status_code: 200
        validate_certs: false
      register: result
      until: result.status == 200
      retries: 10
      delay: 10

    - name: Redeploy all pods in kube-system namespace (including Flannel)
      ansible.builtin.shell: kubectl delete pod -l app=flannel --namespace=kube-system
      become_user: ec2-user
      when: result.status == 200

    - name: Restart all deployments in kube-system namespace
      ansible.builtin.shell: kubectl rollout restart deployment --all --namespace=kube-system
      become_user: ec2-user
      when: result.status == 200

    - name: Restart all deployments in all namespaces
      ansible.builtin.shell: |
        for ns in $(kubectl get namespaces -o custom-columns=":metadata.name"); do
          kubectl rollout restart deployment --all --namespace=$ns
        done
      become_user: ec2-user
      when: result.status == 200
...